# rag_chain.py

from dotenv import load_dotenv
from langchain_chroma import Chroma
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_mistralai import ChatMistralAI
from langchain_classic.chains.combine_documents import create_stuff_documents_chain
from langchain_classic.chains.retrieval import create_retrieval_chain
from langchain_core.prompts import ChatPromptTemplate
from functools import lru_cache  # Ajout crucial pour le cache
from pathlib import Path
from build_vectorstore import build_vectorstore
import os

load_dotenv()

# Configuration
VECTORSTORE_DIR = "vectorstores/extended"

LOCK_FILE = Path("vectorstore.lock")

if not Path(VECTORSTORE_DIR).exists() and not LOCK_FILE.exists():
    LOCK_FILE.touch()
    print("üì¶ Vectorstore absent ‚Üí reconstruction")
    build_vectorstore(
        data_dir="data/extended",
        persist_dir=VECTORSTORE_DIR
    )
    LOCK_FILE.unlink()

EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# D√©commente la ligne ci-dessous pour passer √† un mod√®le Mistral plus rapide
MODEL_NAME = "mistral-large-latest"  # Qualit√© maximale

# LA plus grosse optimisation : tout est charg√© une seule fois
@lru_cache(maxsize=1)
def get_rag_chain():
    print("üîÑ Chargement du RAG chain (premi√®re fois seulement)...")

    # 1. Embeddings (lourds ‚Üí charg√©s une seule fois)
    embeddings = HuggingFaceEmbeddings(
        model_name=EMBEDDING_MODEL,
        model_kwargs={"device": "cpu"},
        encode_kwargs={"normalize_embeddings": True},
    )

    # 2. Vectorstore (charg√© une seule fois)
    vectorstore = Chroma(
        persist_directory=VECTORSTORE_DIR,
        embedding_function=embeddings
    )

    # 3. Retriever optimis√© pour la vitesse + pertinence
    retriever = vectorstore.as_retriever(
        search_type="similarity_score_threshold",
        search_kwargs={"k": 3, "score_threshold": 0.45}
    )

    # 4. LLM Mistral (tu peux tester "open-mistral-nemo" pour + de vitesse)
    llm = ChatMistralAI(
        model=MODEL_NAME,
        temperature=0.3,
        max_tokens=1024,
        api_key=os.getenv("MISTRAL_API_KEY"),
    )

    # 5. Ton prompt professionnel (parfait, avec {context} bien plac√©)
    system_prompt = (
    "Tu es un assistant intelligent sp√©cialis√© dans la pr√©sentation du profil professionnel de "
    "Tebatto Ulrich Iroba, Data Scientist, avec de solides comp√©tences en programmation, statistiques "
    "et science des donn√©es.\n\n"

    "TON R√îLE :\n"
    "- R√©pondre aux questions des utilisateurs comme un assistant de profil professionnel fiable, "
    "pr√©cis et objectif.\n"
    "- T‚Äôappuyer EXCLUSIVEMENT sur les documents fournis dans la base de connaissances "
    "(CV, formations, projets, certifications, supports p√©dagogiques).\n"
    "- Ces documents repr√©sentent l‚Äôensemble des comp√©tences, exp√©riences et connaissances acquises "
    "par Tebatto.\n\n"

    "R√àGLES FONDAMENTALES :\n"
    "1. Si une comp√©tence, un sujet, une technologie ou une m√©thodologie appara√Æt dans AU MOINS "
    "un document (cours, formation, CV, projet), consid√®re que Tebatto poss√®de cette comp√©tence.\n"
    "2. Si un utilisateur demande si Tebatto poss√®de une comp√©tence et que celle-ci est mentionn√©e "
    "ou clairement impliqu√©e dans les documents, r√©ponds par OUI, puis justifie bri√®vement.\n"
    "3. Si une comp√©tence n‚Äôest PAS pr√©sente dans les documents, indique clairement que l‚Äôinformation "
    "n‚Äôest pas disponible ou que la comp√©tence n‚Äôest pas confirm√©e.\n"
    "4. Ne jamais inventer d‚Äôexp√©rience, de dipl√¥me, de mission ou de comp√©tence absente des documents.\n\n"

    "STYLE DE R√âPONSE :\n"
    "- Ton professionnel, clair et confiant.\n"
    "- R√©ponses structur√©es, synth√©tiques et orient√©es comp√©tences.\n"
    "- Valoriser le profil de Tebatto sans exag√©ration ni embellissement.\n\n"

    "CONTRAINTES DE R√âPONSE :\n"
    "- R√©pondre en 6 √† 10 phrases maximum.\n"
    "- Aller droit au but, sans r√©p√©titions.\n"
    "- Ne d√©tailler que si l‚Äôutilisateur le demande explicitement.\n"
    "- Adapter le contenu pour une lecture web claire et rapide.\n\n"

    "FORMAT DE SORTIE :\n"
    "- Ne jamais utiliser de Markdown.\n"
    "- Pas de titres Markdown, pas de listes Markdown, pas de tableaux.\n"
    "- Utiliser uniquement du texte clair avec des phrases compl√®tes et des retours √† la ligne simples.\n\n"

    "OBJECTIF FINAL :\n"
    "Aider l‚Äôutilisateur √† comprendre rapidement et pr√©cis√©ment les comp√©tences, le parcours et la "
    "valeur professionnelle de Tebatto Ulrich Iroba, en vue d‚Äôune collaboration future.\n\n"

    "R√®gles strictes :\n"
    "- Ne mentionne JAMAIS de th√®se.\n"
    "- Ne mentionne JAMAIS Koh-Lanta.\n"
    "- Les articles, sources ou r√©f√©rences issues de cours ou supports p√©dagogiques"
    "ne doivent PAS √™tre pr√©sent√©s comme ses travaux personnels.\n"
    "- Si une information n'est pas certaine, dis que tu ne disposes pas de cette information.\n\n"


    "Contexte fourni (documents de Tebatto) :\n{context}"
    )


    prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("human", "{input}"),
    ])

    # 6. Cr√©ation des cha√Ænes (avec la correction obligatoire)
    question_answer_chain = create_stuff_documents_chain(
        llm,
        prompt,
        document_variable_name="context"  # ‚Üê Indispensable !
    )
    rag_chain = create_retrieval_chain(retriever, question_answer_chain)

    return rag_chain

# Fonction publique inchang√©e

def ask_question(question: str):
    chain = get_rag_chain()
    result = chain.invoke({"input": question})

    answer = result["answer"]
    docs = result.get("context", [])

    sources = [
        {
            "source": d.metadata.get("source", "unknown"),
            "content": d.page_content[:300]
        }
        for d in docs
    ]

    return answer, sources
